{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import optuna\n",
    "from optuna.storages import JournalStorage, JournalFileStorage\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import os\n",
    "from typing import Dict\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from tmu.models.classification.vanilla_classifier import TMClassifier\n",
    "from tmu.preprocessing.standard_binarizer.binarizer import StandardBinarizer\n",
    "from tmu.data import TMUDataset\n",
    "from tmu.composite.components.base import TMComponent\n",
    "from tmu.composite.composite import TMComposite\n",
    "from tmu.composite.config import TMClassifierConfig\n",
    "from tmu.composite.callbacks.base import TMCompositeCallback\n",
    "import logging\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import io\n",
    "import sys\n",
    "sys.path.insert(0,\"..\")\n",
    "from TM_mat_comp.components.domf_component import DomfComponent\n",
    "from TM_mat_comp.components.fft_component import FftComponent\n",
    "from TM_mat_comp.components.std_component import StdComponent\n",
    "from TM_mat_comp.components.psd_component import PsdComponent\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.INFO)\n",
    "logging.getLogger('PIL.PngImagePlugin').setLevel(logging.INFO)\n",
    "_LOGGER = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(TMUDataset):\n",
    "    def __init__(self, dataset_name, use_cache=False):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.use_cache = use_cache\n",
    "        self.classes = {'pulse':0,'31':1,'12':2,'sine':3,'34':4,'11':5,'chirp_uneven':6,'not_sweep':7,'32':8,'40':9}\n",
    "        # {\"11\": 0, \"12\": 1, \"31\": 2, \"32\": 3, \"34\": 4, \"40\": 5, \"chirp_uneven\": 6, \"not_sweep\": 7, \"pulse\": 8, \"sine\": 9}\n",
    "        self.cache_dir = './cache'\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def _cache_path(self, dataset_type):\n",
    "        return os.path.join(self.cache_dir, f\"{self.dataset_name}_{dataset_type}_{self.w}x{self.h}.pkl\")\n",
    "\n",
    "    def _load_from_cache(self, dataset_type):\n",
    "        path = self._cache_path(dataset_type)\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return None\n",
    "\n",
    "    def _save_to_cache(self, dataset_type, data):\n",
    "        path = self._cache_path(dataset_type)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def _transform(self, name, dataset):\n",
    "        return dataset\n",
    "\n",
    "    def _retrieve_dataset(self) -> Dict[str, np.ndarray]:\n",
    "        dataset = {}\n",
    "        for dtype in ['train', 'test']:\n",
    "            if self.use_cache:\n",
    "                cached_data = self._load_from_cache(dtype)\n",
    "                if cached_data:\n",
    "                    dataset.update(cached_data)\n",
    "                    continue\n",
    "            data = self.create_train_data(dtype)\n",
    "            if self.use_cache:\n",
    "                self._save_to_cache(dtype, {f'x_{dtype}': data[0], f'y_{dtype}': data[1]})\n",
    "            dataset[f'x_{dtype}'] = data[0]\n",
    "            dataset[f'y_{dtype}'] = data[1]\n",
    "        return dataset\n",
    " \n",
    "\n",
    "    def create_train_data(self, dataset_type): \n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        main_path = os.path.join('/data', self.dataset_name) \n",
    "\n",
    "        data_dir = os.path.join(main_path, dataset_type)\n",
    "\n",
    "        for class_folder in os.listdir(data_dir):\n",
    "            class_dir = os.path.join(data_dir, class_folder)\n",
    "            for j in os.listdir(class_dir)[0:10]:\n",
    "                class_path = os.path.join(class_dir, j)\n",
    "                mat_data = sio.loadmat(class_path)\n",
    "                X_data.append(mat_data)\n",
    "                y_data.append(self.classes.get(class_folder))\n",
    "\n",
    "        return np.array(X_data), np.array(y_data)\n",
    "\n",
    "class TMCompositeEvaluationCallback(TMCompositeCallback):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.best_acc = 0.0\n",
    "        self.data = data\n",
    "\n",
    "    def on_epoch_end(self, composite, epoch, logs=None):\n",
    "        preds = composite.predict(data=self.data)\n",
    "        acc = (preds == self.data[\"Y\"]).mean()\n",
    "        _LOGGER.info(f\"Epoch {epoch} - Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TMCompositeTuner:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_train,\n",
    "            data_test,\n",
    "            platform=\"CPU\",\n",
    "            max_epochs=200,\n",
    "            n_jobs: int = 1,\n",
    "            callbacks=None,\n",
    "            use_multiprocessing=True,\n",
    "            study_name=\"TMComposite_study\",\n",
    "            target_size = (80,80)\n",
    "    ):\n",
    "        self.data_train = data_train\n",
    "        self.data_test = data_test\n",
    "        self.last_accuracy = 0.0\n",
    "        self.n_components = 1\n",
    "        self.n_jobs = n_jobs\n",
    "        self.study_name = study_name\n",
    "        self.platform = platform\n",
    "        self.max_epochs = max_epochs\n",
    "        self.target_size = target_size\n",
    "        if callbacks is None:\n",
    "            callbacks = []\n",
    "\n",
    "        self.callbacks = callbacks\n",
    "        self.use_multiprocessing = use_multiprocessing\n",
    "\n",
    "    def objective(self, trial: optuna.trial.Trial) -> float:\n",
    "        components_list = []\n",
    "\n",
    "        for i in range(self.n_components):\n",
    "            component_type = trial.suggest_categorical(f'component_type_{i}',\n",
    "                                                       ['DomfComponent'\n",
    "                                                        'FftComponent',\n",
    "                                                        'StdComponent',\n",
    "                                                        'PsdComponent'\n",
    "                                                        ])\n",
    "\n",
    "            num_clauses = trial.suggest_int(f'num_clauses_{i}', 1000, 3000)\n",
    "            T = trial.suggest_int(f'T_{i}', 100, 1500)\n",
    "            s = trial.suggest_float(f's_{i}', 2.0, 15.0)\n",
    "            max_included_literals = trial.suggest_int(f'max_literals_{i}', 16, 64)\n",
    "            weighted_clauses = trial.suggest_categorical(f'weighted_clauses_{i}', [True, False])\n",
    "            epochs = trial.suggest_int(f'epochs_{i}', 1, self.max_epochs)\n",
    "\n",
    "            config = TMClassifierConfig(\n",
    "                number_of_clauses=num_clauses,\n",
    "                T=T,\n",
    "                s=s,\n",
    "                max_included_literals=max_included_literals,\n",
    "                platform=self.platform,\n",
    "                patch_dim=(10, 10),\n",
    "                weighted_clauses=weighted_clauses\n",
    "            )\n",
    "\n",
    "            if component_type == 'DomfComponent':\n",
    "                patch_dim = (trial.suggest_int(f'patch_dim_1_{i}', 1, 10), trial.suggest_int(f'patch_dim_2_{i}', 1, 10))\n",
    "                config.patch_dim = patch_dim\n",
    "                components_list.append(DomfComponent(TMClassifier, config, epochs=epochs, target_size=self.target_size))\n",
    "\n",
    "            elif component_type == 'FftComponent':\n",
    "                patch_dim = (trial.suggest_int(f'patch_dim_1_{i}', 1, 10), trial.suggest_int(f'patch_dim_2_{i}', 1, 10))\n",
    "                config.patch_dim = patch_dim\n",
    "                components_list.append(FftComponent(TMClassifier, config, epochs=epochs, target_size=self.target_size))\n",
    "\n",
    "            elif component_type == 'StdComponent':\n",
    "                patch_dim = (trial.suggest_int(f'patch_dim_1_{i}', 1, 10), trial.suggest_int(f'patch_dim_2_{i}', 1, 10))\n",
    "                config.patch_dim = patch_dim\n",
    "                components_list.append(StdComponent(TMClassifier, config, epochs=epochs, target_size=self.target_size))\n",
    "\n",
    "            elif component_type == 'PsdComponent':\n",
    "                patch_dim = (trial.suggest_int(f'patch_dim_1_{i}', 1, 10), trial.suggest_int(f'patch_dim_2_{i}', 1, 10))\n",
    "                config.patch_dim = patch_dim\n",
    "                components_list.append(PsdComponent(TMClassifier, config, epochs=epochs, target_size=self.target_size))\n",
    "\n",
    "\n",
    "        composite_model = TMComposite(components=components_list, use_multiprocessing=self.use_multiprocessing)\n",
    "\n",
    "        # Training and evaluation\n",
    "        composite_model.fit(\n",
    "            data=self.data_train,\n",
    "            callbacks=self.callbacks\n",
    "        )\n",
    "\n",
    "        preds = composite_model.predict(data=self.data_test)\n",
    "        accuracy = (preds['composite'] == self.data_test['Y'].flatten()).mean()\n",
    "\n",
    "        # Adjust number of components for next trial\n",
    "        if accuracy > self.last_accuracy:\n",
    "            self.n_components += 1\n",
    "        else:\n",
    "            self.n_components = max(1, self.n_components - 1)\n",
    "\n",
    "        self.last_accuracy = accuracy\n",
    "        return accuracy\n",
    "\n",
    "    def save_best_params(self, study, trial, filename=\"best_params.json\"):\n",
    "        best_data = {\n",
    "            'params': study.best_params,\n",
    "            'value': trial.value\n",
    "        }\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(best_data, f)\n",
    "\n",
    "    def gradual_saving_callback(self, study, trial):\n",
    "        # Use np.isclose to handle potential floating-point precision issues\n",
    "        if np.isclose(trial.value, study.best_value, atol=1e-10):\n",
    "            self.save_best_params(study, trial, filename=f\"best_params_trial_{trial.number}.json\")\n",
    "\n",
    "    def retry_optimize(self, study, objective, n_trials, callbacks, max_retries=5, wait_time=2.0):\n",
    "        for _ in range(max_retries):\n",
    "            try:\n",
    "                study.optimize(objective, n_trials=n_trials, callbacks=callbacks)\n",
    "                return\n",
    "            except Exception as e:\n",
    "                if \"database is locked\" in str(e).lower():\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    raise e\n",
    "        raise RuntimeError(\"Max retries reached for database access\")\n",
    "\n",
    "    def tune(self, n_trials: int = 100):\n",
    "        storage = JournalStorage(JournalFileStorage(\"optuna-journal.log\"))\n",
    "        with Parallel(n_jobs=self.n_jobs) as parallel:\n",
    "            if self.n_jobs == 1:\n",
    "                study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(), storage=storage,\n",
    "                                            load_if_exists=True)\n",
    "                self.retry_optimize(study, self.objective, n_trials, [self.gradual_saving_callback])\n",
    "            else:\n",
    "                study = optuna.create_study(study_name=self.study_name, direction='maximize', storage=storage,\n",
    "                                            load_if_exists=True, pruner=optuna.pruners.MedianPruner())\n",
    "                parallel(\n",
    "                    delayed(self.retry_optimize)(study, self.objective, n_trials // self.n_jobs,\n",
    "                                                 [self.gradual_saving_callback])\n",
    "                    for i in range(self.n_jobs)\n",
    "                )\n",
    "\n",
    "        return study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # General hyperparameters\n",
    "    epochs = 2\n",
    "    # device = \"CPU\"\n",
    "    target_size = (31, 31)\n",
    "    # multiprocessing_mode = True\n",
    "    # p_w, p_h = 10, 10\n",
    "    \n",
    "    \n",
    "    \n",
    "    data = DataProcessor(\"data/\", use_cache=False).get()\n",
    "    X_train_org = data[\"x_train\"]\n",
    "    Y_train = data[\"y_train\"]\n",
    "    X_test_org = data[\"x_test\"]\n",
    "    Y_test = data[\"y_test\"]\n",
    "\n",
    "    data_train = dict(\n",
    "        X=X_train_org,\n",
    "        Y=Y_train\n",
    "    )\n",
    "\n",
    "    data_test = dict(\n",
    "        X=X_test_org,\n",
    "        Y=Y_test\n",
    "    )\n",
    "\n",
    "    # Instantiate tuner\n",
    "    tuner = TMCompositeTuner(\n",
    "        data_train=data_train,\n",
    "        data_test=data_test,\n",
    "        max_epochs=epochs,\n",
    "        target_size=target_size,\n",
    "        # callbacks=[TMCompositeEvaluationCallback(data_test)],\n",
    "        n_jobs=1 )\n",
    "\n",
    "\n",
    "    # Specify number of trials (iterations of the tuning process)\n",
    "    n_trials = 3\n",
    "\n",
    "    # Run the tuner\n",
    "    best_params, best_value = tuner.tune(n_trials=n_trials)\n",
    "\n",
    "    # Print out the results\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Value:\", best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
